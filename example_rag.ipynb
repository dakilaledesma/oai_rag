{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxl7G7nfVaGCmwb/Sl32cl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dakilaledesma/oai_rag/blob/main/example_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Retrieval Based Augmentation\n",
        "#### Intro\n",
        "This is a simple retrieval based augmentation workflow. What it does is:\n",
        "1. Read an Excel sheet, where each row in a column is a \"piece of information\"\n",
        "2. Use algorithms* to determine which rows of information are relevant to the prompt being asked to the model\n",
        "3. The most relevant rows of information are fed to ChatGPT as context\n",
        "4. ChatGPT is asked to answer questions based on the context of those most relevant rows of information.\n",
        "\n",
        "The nice thing is that ChatGPT itself can string pieces of information together to make a coherent answer. As a simple example, if some rows pertain to asthma, and other rows are pertinent to upper respiratory infections, and one asks \"How and which upper respiratory infections does asthma exacerbate?\" it will draw information from relevant rows together--even if these rows have no overlap of information--to create a single cohesive answer.\n",
        "\n",
        "*algos are creating embeddings from text using Ada, then using cosine similarity between the prompt asked and the info given"
      ],
      "metadata": {
        "id": "XeU2p1PxLNWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Okay I don't care what do I need to do (Instructions)\n",
        "1. Retrieve an endpoint from OpenAI to be able to fill out details in cell A\n",
        "2. Upload your Excel sheet (on the left side of this Colab page)\n",
        "3. Change info in cell B\n",
        "  - Change the filename to the filename of your file\n",
        "  - Change info_column to the column in the spreadsheet that contains the information you want to feed to ChatGPT\n",
        "4. Change info in cell C, instructions given within the cell\n",
        "5. Go to Runtime -> Run all above (or CTRL+F9/CMD+F9)\n",
        "6. Go to the bottom cell, change the question to what you want. Any time you want to ask a new question or get a new response re-run only that cell (through the play button next to the cell, or SHIFT+ENTER)\n"
      ],
      "metadata": {
        "id": "0bpjHtPIglI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "pip uninstall typing_extensions\n",
        "pip install openai==0.28\n",
        "pip install tiktoken"
      ],
      "metadata": {
        "id": "4PEmlDYtesly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dCcOe178eo16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTmshkcGLGt-"
      },
      "outputs": [],
      "source": [
        "# Cell A: OpenAI configuration\n",
        "openai.api_type = \"azure\"  # For BCBS this was \"azure\" as we have an azure endpoint\n",
        "openai.api_key = \"d0781e8d...\"  # Will look like a garbled mess, ours was \"d0781e8d...\" (not sharing the entire thing for obvious reasons)\n",
        "openai.api_base = \"https://somethingsomethingsomething.openai.azure.com/\"  # Will be a URL, ours was similar to https://somethingsomethingsomething.openai.azure.com/\n",
        "openai.api_version = \"2023-05-15\" # Keep this the same"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B: Excel Sheet filename and column\n",
        "# Change this filename to the file name of your excel sheet\n",
        "df = pd.read_excel(\"Responses.xlsx\")\\\n",
        "\n",
        "# Change this to the column in the Excel sheet where you'd like to feed the GPT\n",
        "# information\n",
        "info_column = \"Question\""
      ],
      "metadata": {
        "id": "_5YtueZAL6bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C: Prompt \"Engineering\"\n",
        "\n",
        "# Change this to how you want the GPT model to answer with your sources of\n",
        "# information. You will want to be as assertive and direct as you feel is\n",
        "# necessary. This will depend on how closely you would like the model to adhere\n",
        "# to these rules. An example may be found below (real example given).\n",
        "assertion_text = \"\"\"You are given information on how our company responds to request for proposal questions. Comments, keywords, and short descriptions describe what the question may be about, and responses are response to that question.\n",
        "Also note that we as a company are trying to differentiate ourselves by highlighting a personal approach that includes consultation, easy reporting solutions, convenience etc. while you formulate potential responses.\n",
        "\n",
        "Please generate us responses given the above as context, noting that we require the content IDs as sources of how you answer that question (multiple content IDs are fine). Please make sure to answer with Content IDs.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-eTE058Rdhrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "df['n_tokens'] = df[info_column].apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "ada_embeddings = []\n",
        "for _, row in df.iterrows():\n",
        "    if row[\"n_tokens\"] < 7800:\n",
        "        response = openai.Embedding.create(\n",
        "            input=row[info_column],\n",
        "            engine=\"text_embedding_ada_002_API_testing\"\n",
        "        )\n",
        "        ada_embeddings.append(response['data'][0]['embedding'])\n",
        "    else:\n",
        "        print(row[\"Content ID\"])\n",
        "        ada_embeddings.append([0] * 1536)\n",
        "\n",
        "df[\"Embeddings\"] = ada_embeddings"
      ],
      "metadata": {
        "collapsed": true,
        "id": "To3h679LN8qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_toks = [v for v in rfp_df[\"n_tokens\"] if v < 7800]\n",
        "_avg = sum(_toks) / len(_toks)\n",
        "\n",
        "def search_docs(df, user_query, top_n=3, to_print=True):\n",
        "    embedding = get_embedding(\n",
        "        user_query,\n",
        "        engine=\"text_embedding_ada_002_API_testing\")\n",
        "    df[\"similarities\"] = df[\"Embeddings\"].apply(lambda x: cosine_similarity(x, embedding))\n",
        "\n",
        "    res = (\n",
        "        df.sort_values(\"similarities\", ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    if to_print:\n",
        "        display(res)\n",
        "    return res"
      ],
      "metadata": {
        "id": "cO1MJC_9QbNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question answer part\n",
        "Ask your question here when you're done setting up the above Excel sheet, settings, etc."
      ],
      "metadata": {
        "id": "A461x0F-iTE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this to your question\n",
        "user_prompt = \"Give me an appropriate response to this question: Blah blah something about anaesthesia???\"\n",
        "\n",
        "res = search_docs(df, user_prompt, top_n=28)\n",
        "toks = 0\n",
        "texts = []\n",
        "incs = []\n",
        "for _, row in res.iterrows():\n",
        "    if toks + row[\"n_tokens\"] > 7800:\n",
        "        continue\n",
        "    else:\n",
        "        toks += row[\"n_tokens\"]\n",
        "\n",
        "    texts.append(row[\"Embedding Text\"])\n",
        "    incs.append(row[\"Content ID\"])\n",
        "\n",
        "# You can change the first part of the following (Give me an appropriate...) if\n",
        "# there is a more appropriate prompt engineering prompt for your use case\n",
        "texts.append(f\"Give me an appropriate response to this question: {user_prompt}\")\n",
        "\n",
        "full_response = \"\"\n",
        "for response in openai.ChatCompletion.create(\n",
        "    engine=\"35_turbo\",\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=2.0,\n",
        "    messages=messages,\n",
        "    stream=True\n",
        "):\n",
        "    resp = response.choices[0].delta.get(\"content\", \"\")\n",
        "    full_response += resp\n",
        "\n",
        "print(full_response)"
      ],
      "metadata": {
        "id": "fthJ8ilHTKDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}